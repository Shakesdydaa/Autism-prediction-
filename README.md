AUTISM PREDICTION USING MACHINE LEARNING Executive Summary This project explores the application of machine learning algorithms in predicting autism spectrum disorder (ASD) using demographic data and standardized questionnaire responses. The objective is to develop a predictive model that can support early screening interventions, particularly in low-resource settings where traditional diagnostic tools may be inaccessible. The data underwent extensive pre-processing and was used to train three classification models: Decision Tree, Random Forest, and XGBoost. Among these, the Random Forest model exhibited the highest accuracy and robustness, making it the most suitable for deployment. This report details the problem definition, data processing steps, model development, evaluation, and recommendations for future improvement. 2. Problem Statement Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition that affects communication, behaviour, and social interaction. Early identification and intervention are critical for improving outcomes, yet access to diagnostic services is limited in many regions. Traditional diagnostic methods are time-consuming, expensive, and require clinical expertise, making early screening challenging, particularly in low-resource settings. This project aims to address this gap by developing an automated machine learning-based classification model that can predict the likelihood of ASD in individuals using questionnaire responses and demographic data. By leveraging data-driven techniques, the model seeks to provide an accurate, scalable, and accessible tool for preliminary autism screening to support timely clinical follow-up and intervention. 3. Target Users Healthcare providers School counsellors Parents and guardians Policymakers in public health Developers of health screening software 4. Main Objective To develop a machine learning-based tool capable of predicting autism in individuals based on questionnaire responses and demographic features. 4.1 Specific Objectives To apply data pre-processing techniques to clean, transform, and balance the dataset for effective training. To train, evaluate and compare multiple machine learning models (Decision Tree, Random Forest, and XGBoost) for their effectiveness in classifying ASD. To select the best-performing model and provide performance metrics, including confusion matrices, to interpret its predictive capability. To export and save the final model for future use in clinical or screening applications. 5. Proposed Solution Develop a machine learning-based predictive model trained on a structured dataset of questionnaire responses and demographic data. The solution includes applying pre-processing techniques, model selection, training, cross-validation, and evaluation, followed by saving the best model for real-world deployment in screening environments. 6. Data Collection and Pre-processing 6.1 Data Source The dataset used for autism prediction was obtained from a structured CSV file from Kaggle. It contains behavioural and demographic variables collected through standardized autism screening questionnaires. These include binary responses (Yes/No or 0/1) to ten questions, alongside metadata such as age, gender, ethnicity, and family history of ASD. 6.2 Data Collection The dataset was collected from anonymized self-reported surveys or clinical records targeting early autism detection. The data represents a balanced mix of ASD-positive and ASD-negative individuals but required further rebalancing due to slight class imbalance. 6.3 Initial Structure and Importation Using Python’s pandas, the CSV file was loaded into a DataFrame for analysis. Exploratory commands such as .info (),. head(), .shape(), and .describe() were used to assess column types, detect missing values, and understand distributions. 6.3.1 Outputs from EDA: Shape: (800, 22) Integer columns: 12 (e.g., A1\_Score to A10\_Score, ID, Class/ASD) Float columns: 2 (age, result) Object columns: 8 (e.g., gender, ethnicity, jaundice, autism, country\_of\_res, used\_app\_before, age\_desc, relation) .info(): All columns are non-null with appropriate data types .describe(): Provided statistical summaries for numeric columns like age and result, with Class/ASD mean ~0.20 indicating class imbalance .head(): Displayed the first five rows to confirm data format 6.4 Insights from Exploratory Data Analysis (EDA) Outliers Detected: Numerical features such as age and result exhibited a few outliers, particularly at the higher ends of the distribution. These were retained as they appeared plausible and did not significantly skew model performance. Class Imbalance Observed: The target variable (Class/ASD) is imbalanced, with fewer positive autism cases compared to negative ones. This warranted the use of SMOTE during pre-processing. Some categorical features, such as used\_app\_before and family history of autism, also showed skewed distributions. Low Correlation Between Features: Correlation analysis revealed that there were no highly correlated numerical features that would necessitate dropping due to redundancy. Categorical Feature Transformation: Label encoding was applied to all categorical fields (e.g., gender, ethnicity, jaundice, relation) to convert them into a format suitable for model training. Visualization Tools Used: Bar plot showing class distribution reveals imbalance. Heatmap shows correlation between features. 6.5 Data Cleaning Steps Dropped irrelevant features: ID and age\_desc were excluded from analysis as they had no predictive value. Converted age to integer to ensure consistency. Handled any missing values by replacing incomplete records (no imputation was necessary due to minimal loss). 6.6 Feature Engineering Label Encoding: Applied to categorical fields like gender, ethnicity, and family history, converting them into machine-readable numeric form. Target Transformation: The Class/ASD label was standardized to binary format for classification purposes. Normalization: Numerical features like age were kept on a natural scale given their limited range, avoiding excessive transformations. 6.7 Addressing Class Imbalance The dataset initially had a slight skew toward the non-ASD class. To improve model learning, SMOTE (Synthetic Minority Oversampling Technique) from the imblearn library was used to synthetically generate minority class samples, resulting in a balanced dataset ready for supervised learning. 7. Model Development 7.1 Model Selection and Hyperparameter Tuning To identify the most effective algorithm for autism prediction, three supervised learning classifiers were evaluated: Decision Tree Classifier: A basic yet interpretable model that splits data based on feature thresholds. Random Forest Classifier: An ensemble of decision trees that improves performance through bagging and reduces overfitting. XGBoost Classifier: A gradient boosting model known for superior accuracy and robustness in handling imbalanced data. 7.2 Training Strategy The pre-processed dataset was split into training and testing sets using an 80:20 ratio. SMOTE was applied to the training set to address class imbalance by synthetically oversampling the minority class (ASD positive). Each model was trained on the same balanced training data to ensure a fair comparison. 7.3 Validation Approach Cross-validation (CV) was performed using 5-fold CV to minimize bias and variance in performance estimation. CV ensured the model generalized well to unseen data and prevented overfitting. 7.3.1 Cross-Validation Results Decision Tree: Cross-validation accuracy = 0.86 Random Forest: Cross-validation accuracy = 0.91 XGBoost: Cross-validation accuracy = 0.90 7.4 Evaluation Metrics Accuracy: Percentage of correctly predicted instances. F1-Score: Harmonic mean of precision and recall, particularly important due to class imbalance. Confusion Matrix: Provided a breakdown of true positives, false positives, true negatives, and false negatives. 7.5 Model Comparison and Evaluation Three supervised learning models (Decision Tree, Random Forest, and XGBoost) were trained and evaluated using 5-fold cross-validation and standard classification metrics. Model Accuracy CV Accuracy Precision Recall F1-Score Decision Tree 0.86 0.86 Moderate Moderate Moderate Random Forest 0.91 0.92 High High High XGBoost 0.90 0.90 High Balanced High 7.6 Evaluation Summary Decision Tree: Performed adequately but exhibited higher false positives and lower generalization. Random Forest: Achieved the highest overall performance, especially with tuned parameters (bootstrap=False, max\_depth=20, n\_estimators=50). It showed superior accuracy and balance across all metrics. XGBoost: Performed closely to Random Forest, with strong balance and robustness, making it a viable alternative where interpretability is needed. 8. Recommendations Given the success of the model, several recommendations were made. In the short term, it is recommended to deploy the Random Forest model via a user-friendly web application using frameworks like Streamlit or Flask. This would allow pilot testing in clinics or schools. In the medium term, the model should be retrained with more diverse and real-world data to improve generalizability. Incorporating interpretability tools such as SHAP or LIME can enhance transparency and trust in the model’s predictions. Long-term recommendations include integrating the tool into national health information systems, in Kenya where there is limited access to pediatric specialists. There is also potential to enhance the model by incorporating audio-visual data, such as speech patterns or facial expressions, to create a multi-modal diagnostic aid. Finally, publishing the tool as open-source software can accelerate its adoption and customization by developers and researchers worldwide 9. Conclusion Based on cross-validation performance and confusion matrix evaluation, the Random Forest model was selected as the best model for deployment due to its high predictive accuracy and balanced classification performance. This model, when exported and deployed, can serve as an assistive tool for early screening of ASD cases, especially in community or primary healthcare settings.
